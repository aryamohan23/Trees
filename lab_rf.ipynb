{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn.apionly as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First, the data. We will be attempting to predict the presidential election results (at the county level) from 2016, measured as 'votergap' = (trump - clinton) in percentage points, based mostly on demographic features of those counties.  Let's quick take a peak at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "elect_df = pd.read_csv(\"data/county_level_election.csv\")\n",
    "elect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split 80/20 train-test\n",
    "Xdf = elect_df[['population','hispanic','minority','female','unemployed','income','nodegree','bachelor','inactivity','obesity','density','cancer']]\n",
    "response = elect_df['votergap']\n",
    "Xtraindf, Xtestdf, ytrain, ytest = train_test_split(Xdf, response, test_size=0.2, random_state=1983)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "What's the basic idea?\n",
    "\n",
    "Bagging alone is not enough randomization, because even after bootstrapping, we are mainly training on the same data points using the same variablesn, and will retain much of the overfitting.\n",
    "\n",
    "So we will build each tree by splitting on \"random\" subset of predictors at each split (hence, each is a 'random tree').  This can't be done in with just one predcitor, but with more predictors we can choose what predictors to split on randomly and how many to do this on.  Then we combine many 'random trees' together by averaging their predictions, and this gets us a forest of random trees: a **random forest**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a hyper-param Grid. We are preparing to use the bootstrap points not used in training for validation.\n",
    "\n",
    "```\n",
    "max_features : int, float, string or None, optional (default=”auto”)\n",
    "- The number of features to consider when looking for the best split.\n",
    "```\n",
    "\n",
    "- `max_features`: Default splits on all the features and is probably prone to overfitting. You'll want to validate on this. \n",
    "- You can \"validate\" on the trees `n_estimators` as well but many a times you will just look for the plateau in the trees as seen below.\n",
    "- From decision trees you get the `max_depth`, `min_samples_split`, and `min_samples_leaf` as well but you could leave those at defaults to get a maximally expanded tree as adding multiple trees will squeeze out the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use itertools product to simulare what the param-grid in cross-validate does for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from \n",
    "# Adventures in scikit-learn's Random Forest by Gregory Saunders\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "param_dict = OrderedDict(\n",
    "    n_estimators = [400, 600, 800],\n",
    "    max_features = [0.2, 0.4, 0.6, 0.8]\n",
    ")\n",
    "\n",
    "param_dict.values()\n",
    "\n",
    "list(product(*param_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the OOB score.\n",
    "\n",
    "Why did we play with `itertools.product`?\n",
    "\n",
    "We have been putting \"validate\" in quotes. This is because the bootstrap gives us left-over points! So we'll now engage in our very own version of a grid-search, done over the out-of-bag scores that `sklearn` gives us for free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE\n",
    "\n",
    "With the \"validation\" based on left out samples from the bootstrap, cross-validate and find the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure ytrain is the correct data type...in case you have warnings\n",
    "#print(yytrain.shape,ytrain.shape,Xtrain.shape)\n",
    "#ytrain = np.ravel(ytrain)\n",
    "\n",
    "#Let's Cross-val. on the two 'hyperparameters' we based our grid on earlier\n",
    "results = {}\n",
    "estimators= {}\n",
    "for ntrees, maxf in product(*param_dict.values()):\n",
    "    params = (ntrees, maxf)\n",
    "    est = RandomForestRegressor(oob_score=True, \n",
    "                                n_estimators=ntrees, max_features=maxf, max_depth=50, n_jobs=-1)\n",
    "    est.fit(Xtraindf, ytrain)\n",
    "    results[params] = est.oob_score_\n",
    "    estimators[params] = est\n",
    "outparams = max(results, key = results.get)\n",
    "outparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf1 = estimators[outparams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can find the **feature importance** of each predictor in this random forest model. Whenever a feature is used in a tree in the forest, the algorithm will log the decrease in the splitting criterion (such as gini). This is accumulated over all trees and reported in `est.feature_importances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rf1.feature_importances_,index=list(Xtrain)).sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our response isn't very symmetric, we may want to suppress outliers by using the `mean_absolute_error` instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of using oob scoring, we could do cross-validation (with GridSearchCV), and a cv of 3 will roughly be comparable (same approximate train-vs.-validation set sizes). But this will take much more time as you are doing the fit 3 times plus the bootstraps. So at least three times as long!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. What are the 3 *hyperparameters* for a random forest (one of the hyperparameters come in many *flavors*)?\n",
    "2. Which hyperparameters need to be tuned? \n",
    "\n",
    "**Answers**\n",
    "1. The 3 hyperparameters are `max_features`,  `max_depth` (or something related, like `min_samples_split`, so control the complexity of each tree), and `n_estimators` (in classification, the splitting criteria is another one you could consider)\n",
    "\n",
    "2. `max_features` and  `max_depth` need to be tuned, while the higher `n_estimators` the better (though there is clearly diminishing return, so just use someting 'largish' in the hundreds).  The best tuned `max_features` and `max_depth` will depend on the number of predictors you are considering, the number of observations in the training set, and whether it is a regression or a classification problem.  There are lots of rules of thumb out there, but cross-validate if you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing error as a function of the proportion of predictors at each split\n",
    "\n",
    "Let's look to see how `max_features` affects performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html\n",
    "\n",
    "feats = param_dict['max_features']\n",
    "# \n",
    "error_rate = OrderedDict((label, []) for label in feats)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 200\n",
    "step_estimators = 100\n",
    "num_steps = 6\n",
    "max_estimators = min_estimators + step_estimators*num_steps\n",
    "for label in feats:\n",
    "    for i in range(min_estimators, max_estimators+1, step_estimators):\n",
    "        clf = RandomForestRegressor(oob_score=True, max_features=label)\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(Xtraindf, ytrain)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
